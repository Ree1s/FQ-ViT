ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 12.666 (Best: 12.666 @epoch 1)
Forward: 5.41s

Saving...
Total: 5.41s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 12.666 (Best: 12.666 @epoch 1)
Forward: 5.46s

Saving...
Total: 5.46s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 12.666 (Best: 12.666 @epoch 1)
Forward: 5.37s

Saving...
Total: 5.37s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.394 (Best: 38.394 @epoch 1)
Forward: 5.36s

Saving...
Total: 5.36s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.394 (Best: 38.394 @epoch 1)
Forward: 5.47s

Saving...
Total: 5.47s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.394 (Best: 38.394 @epoch 1)
Forward: 5.40s

Saving...
Total: 5.40s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): VisionTransformer(
    (linear_encoding): Linear(in_features=576, out_features=576, bias=True)
    (mlp_head): Sequential(
      (0): Linear(in_features=576, out_features=2304, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=2304, out_features=576, bias=True)
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (linear1): Linear(in_features=576, out_features=2304, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=2304, out_features=576, bias=True)
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.394 (Best: 38.394 @epoch 1)
Forward: 5.88s

Saving...
Total: 5.88s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Calibration:

Evaluation:
[Set5 x2]	PSNR: 38.394 (Best: 38.394 @epoch 2)
Forward: 6.02s

Saving...
Total: 6.02s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.104 (Best: 38.104 @epoch 1)
Forward: 10.80s

Saving...
Total: 10.80s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.394 (Best: 38.394 @epoch 1)
Forward: 5.87s

Saving...
Total: 5.87s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x2]	PSNR: 38.104 (Best: 38.104 @epoch 1)
Forward: 20.63s

Saving...
Total: 20.63s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=3)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=3)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x3]	PSNR: 34.841 (Best: 34.841 @epoch 1)
Forward: 4.01s

Saving...
Total: 4.01s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=3)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x3]	PSNR: 34.536 (Best: 34.536 @epoch 1)
Forward: 5.96s

Saving...
Total: 5.96s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
        (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x4]	PSNR: 32.655 (Best: 32.655 @epoch 1)
Forward: 3.49s

Saving...
Total: 3.49s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
        (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x4]	PSNR: 32.283 (Best: 32.283 @epoch 1)
Forward: 4.55s

Saving...
Total: 4.55s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
        (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): PixelShuffle(upscale_factor=2)
      )
      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)

Evaluation:
[Set5 x4]	PSNR: 32.283 (Best: 32.283 @epoch 1)
[Set14 x4]	PSNR: nan (Best: nan @epoch 1)
Forward: 5.93s

Saving...
Total: 5.93s

ipt(
  (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (head): ModuleList(
    (0): Sequential(
      (0): QConv2d(
        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (quantizer): UniformQuantizer()
      )
      (1): ResBlock(
        (body): Sequential(
          (0): QConv2d(
            64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)
            (quantizer): UniformQuantizer()
          )
          (1): ReLU(inplace=True)
          (2): QConv2d(
            64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)
            (quantizer): UniformQuantizer()
          )
        )
      )
      (2): ResBlock(
        (body): Sequential(
          (0): QConv2d(
            64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)
            (quantizer): UniformQuantizer()
          )
          (1): ReLU(inplace=True)
          (2): QConv2d(
            64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)
            (quantizer): UniformQuantizer()
          )
        )
      )
    )
  )
  (body): QVisionTransformer(
    (qact_input): QAct(
      (quantizer): UniformQuantizer()
    )
    (linear_encoding): QLinear(
      in_features=576, out_features=576, bias=True
      (quantizer): UniformQuantizer()
    )
    (mlp_head): Sequential(
      (0): QLinear(
        in_features=576, out_features=2304, bias=True
        (quantizer): UniformQuantizer()
      )
      (1): Dropout(p=0, inplace=False)
      (2): ReLU()
      (3): QLinear(
        in_features=2304, out_features=576, bias=True
        (quantizer): UniformQuantizer()
      )
      (4): Dropout(p=0, inplace=False)
    )
    (query_embed): Embedding(1, 147456)
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=576, out_features=576, bias=False)
          )
          (qact_q1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_q2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_k2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact_v): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact1): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact2): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact3): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact4): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact5): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact6): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact7): QAct(
            (quantizer): UniformQuantizer()
          )
          (qact8): QAct(
            (quantizer): UniformQuantizer()
          )
          (linear1): QLinear(
            in_features=576, out_features=2304, bias=True
            (quantizer): UniformQuantizer()
          )
          (dropout): Dropout(p=0, inplace=False)
          (linear2): QLinear(
            in_features=2304, out_features=576, bias=True
            (quantizer): UniformQuantizer()
          )
          (norm1): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm2): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (norm3): QIntLayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
          (dropout3): Dropout(p=0, inplace=False)
        )
      )
    )
    (position_encoding): LearnedPositionalEncoding(
      (pe): Embedding(256, 576)
    )
    (qact_pos): QAct(
      (quantizer): UniformQuantizer()
    )
    (qact1): QAct(
      (quantizer): UniformQuantizer()
    )
    (dropout_layer1): Dropout(p=0, inplace=False)
  )
  (tail): ModuleList(
    (0): Sequential(
      (0): Upsampler(
        (0): QConv2d(
          64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (quantizer): UniformQuantizer()
        )
        (1): PixelShuffle(upscale_factor=2)
        (2): QConv2d(
          64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (quantizer): UniformQuantizer()
        )
        (3): PixelShuffle(upscale_factor=2)
      )
      (1): QConv2d(
        64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (quantizer): UniformQuantizer()
      )
    )
  )
)

Evaluation:
[Set5 x4]	PSNR: 32.300 (Best: 32.300 @epoch 1)
Forward: 14.48s

Saving...
Total: 14.48s

